\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{mathtools,amssymb}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{layout}
\usepackage{geometry}
\usepackage{listings}
\usepackage{color}
\usepackage{pdfpages}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand*{\vertbar}[1][10ex]{\rule[-1ex]{0.5pt}{#1}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand{\norm}[2]{\left\lVert#1\right\rVert_#2}
\newcommand{\abs}[1]{\lvert#1\rvert}

\title{STATS370: Final Project}
\author{Erich Trieschman}
\date{2022 Fall Quarter}


\newcommand{\userMarginInMm}{10}
\geometry{
 left=12 mm,
 right=12 mm,
 top=20 mm,
 bottom=20 mm,
 footskip=5mm}

\newcommand*\circled[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {#1}}}}
\newcommand*\bspace{$\; \bullet \;$}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
    
\lstset{frame=tb, language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3}

\begin{document}
\maketitle

% \tableofcontents

\section*{Introduction}
\begin{itemize}
  \item Compute quantiles, plot histograms, obtain means, variances and correlation matrix.
  \item Check convergence of the chain: does the variance of the distribution stabilize? Run two markov chains, how do they compare?
  \item Hamiltonian: Parameter search. Implement over M, eps, L. See section 3.5 for tuning
  \item Gibbs: basically show the math
  \item Metropolis Hasting: choice of candidate distribution. Make sure domain is right for each, could optimize parameters of the candidate distributions
  \item Importance sampling: TBD
\end{itemize}

\section{Metropolis Hasting}
\subsection*{Implementation}
\begin{itemize}
  \item Candidate distribution selection, including log-normal for ss with change of varibles transformation
  \item Grid search for potential parameters within chosen proposal distribution
\end{itemize}

\subsection*{Design choices, tuning, and scalability}
\subsection*{Results}



% GIBBS
\section{Gibbs sampling}
\subsection*{Implementation}
Posterior conditioned on each parameter
\begin{align*}
  p(\theta[i] | Y, \theta[-i]) = \frac{p(\theta[i], \theta[-i] | Y)}{p(\theta[-i] | Y)} = f(\theta[i]) \propto p(\theta | Y) \textrm{ with fixed } \theta[-i], Y
\end{align*}

\subsection*{Design choices, tuning, and scalability}
\subsection*{Results}

% HMC
\section{Hamiltonian Monte Carlo}
\subsection*{Implementation}
\begin{itemize}
  \item Boundary constraints using Rollback HMC: https://arxiv.org/pdf/1709.02855.pdf and Reflection HMC: https://people.cs.umass.edu/~domke/papers/2015nips1.pdf
\end{itemize}

\begin{align*}
  H(q, p) =& U(q) + K(p) \textrm{, Hamiltonian function}\\
  & \textrm{with $U(\cdot)$, potential energy, $K(\cdot)$, kinetic energy, $q \in \mathbb{R}^k$, position, $p \in \mathbb{R}^k$ momentum}\\
  & \textrm{let } U(\theta) := -\log f(\theta) \textrm{, where } p(\theta | Y) = \frac{1}{C}f(\theta) \textrm{, and } p \sim N(0, \textbf{M})
\end{align*}

\subsection*{Design choices, tuning, and scalability}
\subsection*{Results}


% IMPORTANCE SAMPLING
\section{Importance sampling}
\subsection*{Implementation}
Here I implement importance sampling with normalization. This technique allows me to draw samples, $\theta_i$, from a trial distribution, $q(\theta)$, and weight those samples to reflect my target distribution, $p(\theta \mid Y)$. Normalization allows me to weight samples based on a function proportional to my target distribution, $f(\theta)$, up to an integrating constant (i.e., $p(\theta \mid Y) = Cf(\theta))$.

To implement this method, I observe that my target distribution can be re-written as the outcome of sequential posterior distribution calculations. Specifically
\begin{align*}
  p(\theta \mid Y) = p(\sigma^2, \tau, \mu, \gamma \mid Y) &\propto\; p(\sigma^2) p(\tau) p(\mu) p(\gamma)\times \prod_{i\in g_1, g_2, g_3, g_4} L(y_i \mid \sigma^2, \tau, \mu, \gamma)\\
  &\propto\; (\sigma^2) p(\mu) p(\gamma) \times \prod_{i\in g_1, g_2} L(y_i \mid \sigma^2, \mu, \gamma) \times p(\tau) \times \prod_{i\in g_3, g_4} L(y_i \mid \sigma^2, \tau, \mu, \gamma)\\
  &\propto\; p(\sigma^2, \mu, \gamma \mid Y_{1,2}) \times p(\tau) \times \prod_{i\in g_3, g_4} L(y_i \mid \sigma^2, \tau, \mu, \gamma)
\end{align*}
I next observe that the posterior, $p(\sigma^2, \mu, \gamma \mid Y_{1,2})$ can be computed in closed form.
\begin{align*}
  q(\sigma^2, \mu, \gamma \mid Y_{1,2}) &= q(\mu, \gamma \mid Y_{1,2}) \times q(\sigma^2 \mid \mu, \gamma, Y_{1,2}) \textrm{, where}\\
  q(\mu, \gamma \mid Y_{1,2}) &\sim t_4\left(\mu= \left(\begin{matrix*}
    \overline{Y_1}\\ \overline{Y_2} \end{matrix*}\right), \Sigma=\frac{\sum_{i\in g_1}\lVert y_i - \overline{Y_1} \rVert^2 + \sum_{i \in g_2}\lVert y_i - \overline{Y_2}\rVert^2}{\nu}\left(\begin{matrix*}
      n_1^{-1} & 0 & 0 & 0 \\ 0 & n_1^{-1} & 0 & 0 \\ 0 & 0 & n_2^{-1} & 0 \\ 0 & 0 & 0 & n_2^{-1}
    \end{matrix*}\right), \nu=2(n_1 + n_2) - 4\right)\\
  q(\sigma^2 \mid \mu, \gamma, Y_{1,2}) &\sim InvGamma\left(\alpha=n_1 + n_2, \beta = \frac{1}{2}\sum_{i\in g_1}\lVert y_i - \mu\rVert^2 + \sum_{i\in g_2}\lVert y_i - \gamma\rVert^2\right)
\end{align*}
Lastly, I note that for a trial density $q(\theta) := p(\sigma^2, \mu, \gamma \mid Y_{1,2}) \times p(\tau)$, I can estimate unnormalized sample weights in a straightforward manner. Notably
\begin{align*}
  w_i := \frac{p(\theta_i)}{q(\theta_i)} = \frac{p(\sigma^2, \tau, \mu, \gamma \mid Y)}{p(\sigma^2, \mu, \gamma \mid Y_{1,2}) \times p(\tau)} \propto\; \frac{p(\sigma^2, \mu, \gamma \mid Y_{1,2}) \times p(\tau) \times \prod_{i\in g_3, g_4} L(y_i \mid \sigma^2, \tau, \mu, \gamma)}{p(\sigma^2, \mu, \gamma \mid Y_{1,2})\times p(\tau)} = \prod_{i\in g_3, g_4} L(y_i \mid \sigma^2, \tau, \mu, \gamma) =: u_i
\end{align*}
Using the normalization method proved in class lecture, it follows cleanly that $w_i = u_i / \sum_{j=1}^nu_j$. My sampling method is therefore
\begin{algorithm}
\caption{Importance sampling}
  \begin{algorithmic}[1]
    \For{t=1,\dots,n}
      \State $\theta_i \sim q(\theta) := p(\sigma^2, \mu, \gamma \mid Y_{1,2})= q(\mu, \gamma \mid Y_{1,2}) \times q(\sigma^2 \mid \mu, \gamma, Y_{1,2})$
      \State $u_i = f(\theta_i \mid Y) / q(\theta_i)$  \Comment{$f(\theta_i \mid Y) \propto p(\theta_i \mid Y)$}
    \EndFor
    \State $\textbf{w} = \textbf{u} / \sum_{j=1}^n u_j$ \Comment{$\textbf{u}$ is a vector of all $u_i$}
    \State $\hat{\boldsymbol{\theta}} = \boldsymbol{\theta}*\textbf{w}$ \Comment{element-wise product; $\boldsymbol{\theta}$ is a vector of all $\theta_i$} 
  \end{algorithmic}
\end{algorithm}
\subsection*{Design choices, tuning, and scalability}
\subsection*{Results}

\section{Conclusion}
PLACEHOLDER -- Compare methods

\section{Appendix}
\subsection{Derivation of the posterior, $p(\theta \mid Y)$}
\subsubsection{Provided models for gene expression}
\begin{align*}
  (y_i | g_i = 1) &\sim N(\mu, \sigma^2 I)\\
  (y_i | g_i = 2) &\sim N(\gamma, \sigma^2 I)\\
  (y_i | g_i = 3) &\sim N(\frac{1}{2}(\mu + \gamma), \sigma^2 I)\\
  (y_i | g_i = 4) &\sim N(\tau\mu + (1-\tau)\gamma, \sigma^2 I)
\end{align*}
\subsubsection{Provided priors for gene expression models}
\begin{align*}
  \theta &= (\sigma^2, \tau, \mu_1, \mu_2, \gamma_1, \gamma_2)\\
  p(\sigma^2) &\propto \frac{1}{\sigma^2}\\
  p(\tau) &\sim Unif[0, 1]\\
  p(\mu) &= p(\mu_1, \mu_2) \propto 1 \textrm{ (improper uniform)}\\
  p(\gamma) &= p(\gamma_1, \gamma_2) \propto 1 \textrm{ (improper uniform)}\\
\end{align*}
\subsubsection{Derivation of likelihood and posterior distribution}
\begin{align*}
  L(Y | \theta) =& \prod_{i=1}^n p(y_i | \theta) \textrm{, for } Y = (y_i, \dots, y_n)\\
  =& \prod_{i \in g_1} \frac{1}{\sqrt{2\pi}}((\sigma^2)^2)^{-\frac{1}{2}} \exp[-\frac{1}{2\sigma^2} (y_i - \mu)^T(y_i - \mu)] \times  
     \prod_{i\in g_2} \frac{1}{\sqrt{2\pi}}((\sigma^2)^2)^{-\frac{1}{2}} \exp[-\frac{1}{2\sigma^2} (y_i - \gamma)^T(y_i - \gamma)]\\
  &\times \prod_{i\in g_3} \frac{1}{\sqrt{2\pi}}((\sigma^2)^2)^{-\frac{1}{2}} \exp[-\frac{1}{2\sigma^2} (y_i - \frac{1}{2}(\mu +  
    \gamma))^T(y_i - \frac{1}{2}(\mu + \gamma))] \\
  &\times \prod_{i\in g_4} \frac{1}{\sqrt{2\pi}}((\sigma^2)^2)^{-\frac{1}{2}} \exp[-\frac{1}{2\sigma^2} (y_i - (\tau\mu + (1-\tau)\gamma))^T(y_i - (\tau\mu + (1-\tau)\gamma))] \\
  =& \left(\frac{1}{\sigma^2\sqrt{2\pi}}\right)^n \exp[-\frac{1}{2\sigma^2}(\sum_{i\in g_1}(y_i - \mu)^T(y_i -   \mu) + \sum_{i\in g_2}(y_i - \gamma)^T(y_i - \gamma)\\
  &+ \sum_{i\in g_3}(y_i - \frac{1}{2}(\mu +  
  \gamma))^T(y_i - \frac{1}{2}(\mu + \gamma)) + \sum_{i\in g_4}(y_i - (\tau\mu + (1-\tau)\gamma))^T(y_i - (\tau\mu + (1-\tau)\gamma)))]\\\\
  p(\theta | Y) \propto&\; p(\theta) L(Y | \theta)
\end{align*}



\subsection{Gibbs sampling}
\subsubsection{Derivation of $p(\tau \mid \sigma^2, \mu, \gamma, Y)$}
\begin{align*}
  p(\tau | Y, \theta[-\tau]) &\propto p(\theta)L(Y | \theta)\\  
  & \propto p(\tau)\prod_{i\in g_4} \frac{1}{\sigma^2\sqrt{2\pi}} \exp[-\frac{1}{2\sigma^2} (y_i - (\tau\mu + (1-\tau)\gamma))^T(y_i - (\tau\mu + (1-\tau)\gamma))]\\
  &\propto \exp[-\frac{1}{2\sigma^2}\sum_{i\in g_4}(y_i - (\tau\mu + (1-\tau)\gamma))^T(y_i - (\tau\mu + (1-\tau)\gamma))] \textrm{, for } \tau \in [0,1]\\
  &\propto \exp[-\frac{1}{2\sigma^2}\sum_{i\in g_4}(y_i^Ty_i - 2y_i^T(\tau\mu + (1-\tau)\gamma) + (\tau\mu + (1-\tau)\gamma)^T(\tau\mu + (1-\tau)\gamma))] \textrm{, for } \tau \in [0,1]\\
  &\propto \exp[-\frac{1}{2\sigma^2}\sum_{i\in g_4} \tau^2(\mu^T\mu - 2\mu^T\gamma + \gamma^T\gamma) - 2\tau(y_i^T\mu - y_i^T\gamma - \mu^T\gamma + \gamma^T\gamma)]\textrm{, for } \tau \in [0,1]\\
  &\propto \exp[-\frac{1}{2\sigma^2}(n_4 \tau^2(\mu - \gamma)^T(\mu - \gamma) - 2\tau(\mu - \gamma)^T[\sum_{i\in g_4}(y_i - \gamma) \textrm{, for } \tau \in [0,1]\\
  &\propto \exp\left[-\frac{1}{2}*\frac{n_4(\mu - \gamma)^T(\mu - \gamma)}{\sigma^2}\left(\tau - \frac{(\mu - \gamma)^T(\sum_{i\in g_4}(y_i - \gamma))}{n_4(\mu - \gamma)^T(\mu - \gamma)}\right)^2\right] \textrm{, for } \tau \in [0,1]\\
  &\sim Norm\left(\mu=\frac{(\mu - \gamma)^T(\sum_{i\in g_4}(y_i - \gamma))}{n_4(\mu - \gamma)^T(\mu - \gamma)}, \sigma^2=\frac{\sigma^2}{n_4(\mu - \gamma)^T(\mu - \gamma)}\right) \textrm{, truncated to } [0,1]
\end{align*}

\subsubsection*{Derivation of $p(\sigma^2 \mid \tau, \mu, \gamma, Y)$}
\begin{align*}
  p(\sigma^2 \mid Y, \theta[-\sigma^2]) \propto& p(\theta)L(Y | \theta) \propto p(\sigma^2) \prod_{i=1}^n p(y_i | \theta)\\
  \propto& \frac{1}{\sigma^2} * \left(\frac{1}{\sigma^2}\right)^n \exp\left[-\frac{1}{2\sigma^2}M\right] \textrm{, where } M = \\
  &\;\;\; \sum_{i\in g_1}(y_i - \mu)^T(y_i -   \mu) + \sum_{i\in g_2}(y_i - \gamma)^T(y_i - \gamma)+ \sum_{i\in g_3}(y_i - \frac{1}{2}(\mu +  
  \gamma))^T(y_i - \frac{1}{2}(\mu + \gamma))\\
  &\;\;\; + \sum_{i\in g_4}(y_i - (\tau\mu + (1-\tau)\gamma))^T(y_i - (\tau\mu + (1-\tau)\gamma))\\
  \propto& (\sigma^2)^{-n - 1}\exp\left[-\frac{M}{2}\frac{1}{\sigma^2}\right]\\
  \sim& InvGamma\left(\alpha=n, \beta = \frac{M}{2}\right)
\end{align*}

\subsubsection{Derivation of $p(\mu \mid \sigma^2, \tau, \gamma, Y)$}
\begin{align*}
  p(\mu | Y, \theta[-\mu]) \propto& p(\theta)L(Y | \theta)\\
  \propto& \prod_{i \in g_1} \frac{1}{\sigma^2\sqrt{2\pi}} \exp[-\frac{1}{2\sigma^2} (y_i - \mu)^T(y_i - \mu)]
    \times \prod_{i\in g_3} \frac{1}{\sigma^2\sqrt{2\pi}} \exp[-\frac{1}{2\sigma^2} (y_i - \frac{1}{2}(\mu +  
    \gamma))^T(y_i - \frac{1}{2}(\mu + \gamma))] \\
  &\times \prod_{i\in g_4} \frac{1}{\sigma^2\sqrt{2\pi}} \exp[-\frac{1}{2\sigma^2} (y_i - (\tau\mu + (1-\tau)\gamma))^T(y_i - (\tau\mu + (1-\tau)\gamma))]\\
  \propto& \exp[-\frac{1}{2\sigma^2}(n_1\mu^T\mu - 2\mu^T\left(\sum_{i\in g_1}y_i\right) - \mu^T\left(\sum_{i\in g_3}y_i\right) + \frac{n_3}{4}\mu^T\mu + \frac{n_3}{2}\mu^T\gamma\\
  &-2\tau\mu^T\left(\sum_{i\in g_4}y_i\right) + \tau^2n_4\mu^T\mu + 2\tau(1 - \tau)n_4\mu^T\gamma )]\\
  \propto& \exp\left[-\frac{1}{2\sigma^2}(n_1 + \frac{n_3}{4} + n_4\tau^2)\left(\mu^T\mu - 2\mu^T\frac{\sum_{i \in g_1}y_i + \frac{1}{2}\sum_{i\in g_3}y_i + \tau\sum_{i\in g_4}y_i - (\frac{n_3}{4} + n_4\tau(1 - \tau))\gamma}{n_1 + \frac{n_3}{4} + n_4\tau^2} \right)\right] \\
  \propto& \exp\left[-\frac{1}{2\phi^2}(\mu - \psi)^T(\mu - \psi)\right] \textrm{, where }\\
  &\;\;\; \psi = \frac{\sum_{i \in g_1}y_i + \frac{1}{2}\sum_{i\in g_3}y_i + \tau\sum_{i\in g_4}y_i - (\frac{n_3}{4} + n_4\tau(1 - \tau))\gamma}{n_1 + \frac{n_3}{4} + n_4\tau^2} \textrm{, } \phi^2 = \frac{\sigma^2}{n_1 + \frac{n_3}{4} + n_4\tau^2}\\
  \sim& N(\mu = \psi, \Sigma = \phi^2 I)
\end{align*}

\subsubsection{Derivation of $p(\gamma \mid \sigma^2, \tau, \mu, Y)$}
By symmetry with posterior conditional probability of $\mu$,
\begin{align*}
  p(\gamma | Y, \theta[-\gamma]) \sim& N(\mu = \psi', \Sigma = \phi'^2 I) \textrm{, where }\\
  &\;\;\; \psi' = \frac{\sum_{i \in g_2}y_i + \frac{1}{2}\sum_{i\in g_3}y_i + (1-\tau)\sum_{i\in g_4}y_i - (\frac{n_3}{4} + n_4\tau(1 - \tau))\mu}{n_2 + \frac{n_3}{4} + n_4(1-\tau)^2} \textrm{, } \phi'^2 = \frac{\sigma^2}{n_2 + \frac{n_3}{4} + n_4(1-\tau)^2}
\end{align*}

\subsection{Importance sampling}
\subsubsection{Derivation of $p(\sigma^2 \mid \mu, \gamma, Y_{1,2})$}
\begin{align*}
  q(\sigma^2 \mid \mu, \gamma, Y_{1,2})\propto&\;\; \frac{1}{\sigma^2} * \left(\frac{1}{\sigma^2}\right)^{n_1 + n_2} \exp\left[-\frac{1}{2\sigma^2}M\right] \textrm{, where } M = \sum_{i\in g_1}(y_i - \mu)^T(y_i -   \mu) + \sum_{i\in g_2}(y_i - \gamma)^T(y_i - \gamma)\\
  \propto&\;\; (\sigma^2)^{-n_1 - n_2 - 1}\exp\left[-\frac{M}{2}\frac{1}{\sigma^2}\right]\\
  \sim& InvGamma\left(\alpha=n_1 + n_2, \beta = \frac{M}{2}\right)
\end{align*}

\subsubsection{Derivation of $p(\mu, \gamma \mid Y_{1,2})$}
\begin{align*}
  q(\mu, \gamma \mid Y_{1,2}) =& \int_0^\infty q(\mu, \gamma, \sigma^2 \mid Y_{1,2})d\sigma^2\\
  \propto&\; \int_0^\infty\left(\frac{1}{\sigma^2}\right)^{n_1 + n_2 + n_3 + 1} \exp\left[-\frac{1}{2\sigma^2}(\sum_{i\in g_1} \lVert y_i - \mu\rVert^2 + \sum_{i\in g_2}\lVert y_i - \gamma \rVert^2\right]d\sigma^2\\
  \propto&\; \Gamma(n_1 + n_2) \left(-\frac{1}{2}  \sum_{i\in g_1} \lVert y_i - \mu\rVert^2 + \sum_{i\in g_2}\lVert y_i - \gamma \rVert^2\right)^{-(n_1 + n_2)}, \textrm{ where } \frac{\Gamma(\alpha)}{\beta^\alpha} = \int_0^\infty (\sigma^2)^{-\alpha - 1} \exp\left(\frac{\beta}{\sigma^2}\right)d\sigma^2\\
  \propto&\; (M + n_1\lVert \overline{Y_1} - \mu \rVert^2 + n_2\lVert \overline{Y_2} - \gamma \rVert^2)^{-(n_1 + n_2)}, \textrm{ where } M = \sum_{i\in g_1}\lVert y_i - \overline{Y_1} \rVert^2 + \sum_{i\in g_2}\lVert y_i - \overline{Y_2} \rVert^2\\
  \propto&\; \left(M + \left[\left(\begin{matrix}\mu\\ \gamma\end{matrix}\right) - \left(\begin{matrix*}\overline{Y_1}\\ \overline{Y_2}\end{matrix*}\right)\right]^T \left(\begin{matrix*} n_1 & 0 & 0 & 0 \\ 0 & n_1 & 0 & 0 \\ 0 & 0 & n_2 & 0 \\ 0 & 0 & 0 & n_2 \end{matrix*}\right) \left[\left(\begin{matrix}\mu\\ \gamma\end{matrix}\right) - \left(\begin{matrix*}\overline{Y_1}\\ \overline{Y_2}\end{matrix*}\right)\right]\right)^{-(n_1 + n_2)}\\
  \propto&\; \left(1 + \frac{1}{2(n_1 + n_2) - 4}\times\frac{2(n_1 + n_2) - 4}{M}\left[\left(\begin{matrix}\mu\\ \gamma\end{matrix}\right) - \left(\begin{matrix*}\overline{Y_1}\\ \overline{Y_2}\end{matrix*}\right)\right]^T \left(\begin{matrix*} n_1^{-1} & 0 & 0 & 0 \\ 0 & n_1^{-1} & 0 & 0 \\ 0 & 0 & n_2^{-1} & 0 \\ 0 & 0 & 0 & n_2^{-1} \end{matrix*}\right)^{-1} \left[\left(\begin{matrix}\mu\\ \gamma\end{matrix}\right) - \left(\begin{matrix*}\overline{Y_1}\\ \overline{Y_2}\end{matrix*}\right)\right]\right)^{-(n_1 + n_2)}\\
  \propto&\; \left[1 + \frac{1}{\nu}(x - \eta)^T\Sigma^{-1}(x - \eta)\right]^{-\frac{\nu + 4}{2}}, \textrm{where }\\
  &\;\; \nu=2(n_1 + n_2) + 4, \Sigma=\frac{M}{2(n_1 + n_2) - 4}\left(\begin{matrix*} n_1^{-1} & 0 & 0 & 0 \\ 0 & n_1^{-1} & 0 & 0 \\ 0 & 0 & n_2^{-1} & 0 \\ 0 & 0 & 0 & n_2^{-1} \end{matrix*}\right), \eta=\left(\begin{matrix*}\overline{Y_1}\\ \overline{Y_2}\end{matrix*}\right)\\
  q(\mu, \gamma \mid Y_{1,2}) &\sim t_4\left(\mu= \left(\begin{matrix*}
    \overline{Y_1}\\ \overline{Y_2} \end{matrix*}\right), \Sigma=\frac{\sum_{i\in g_1}\lVert y_i - \overline{Y_1} \rVert^2 + \sum_{i \in g_2}\lVert y_i - \overline{Y_2}\rVert^2}{\nu}\left(\begin{matrix*}
      n_1^{-1} & 0 & 0 & 0 \\ 0 & n_1^{-1} & 0 & 0 \\ 0 & 0 & n_2^{-1} & 0 \\ 0 & 0 & 0 & n_2^{-1}
    \end{matrix*}\right), \nu=2(n_1 + n_2) - 4 \right)
\end{align*}
\end{document}
